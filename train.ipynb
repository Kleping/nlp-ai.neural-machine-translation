{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 500\n",
    "latent_dim = 128\n",
    "# num_data = 100*batch_size\n",
    "language_tag = 'en'\n",
    "data_path = 'data/' + language_tag + '/encoded.txt'\n",
    "model_name = 'nmt'\n",
    "validation_split = .2\n",
    "voc_size_source = 100\n",
    "voc_size_target = 100 + 2\n",
    "i_bos = voc_size_source\n",
    "i_eos = voc_size_source + 1\n",
    "voc_size_source += 1\n",
    "voc_size_target += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def encode_target(text):\n",
    "    return str(i_bos) + ' ' + text + ' ' + str(i_eos)\n",
    "\n",
    "\n",
    "def find_max_seq_data_len(data):\n",
    "    max_source_seq_len = 0\n",
    "    max_target_seq_len = 0\n",
    "    for sample in data:\n",
    "        source, target = sample.split(\"\\t\")\n",
    "\n",
    "        source_len = len(source.split())\n",
    "        if source_len > max_source_seq_len:\n",
    "            max_source_seq_len = source_len\n",
    "\n",
    "        target_len = len(encode_target(target).split())\n",
    "        if target_len > max_target_seq_len:\n",
    "            max_target_seq_len = target_len\n",
    "\n",
    "    return max_source_seq_len, max_target_seq_len\n",
    "\n",
    "\n",
    "def split_data(data):\n",
    "    data_validation = data[-int(validation_split * len(data)):]\n",
    "    data_train = data[:int(len(data) - len(data_validation))]\n",
    "    return data_train, data_validation\n",
    "\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read().split(\"\\n\")\n",
    "    # data = data[:min(len(data), num_data)]\n",
    "    print(len(data))\n",
    "\n",
    "\n",
    "max_source_seq_len, max_target_seq_len = find_max_seq_data_len(data)\n",
    "rn.shuffle(data)\n",
    "data_train, data_valid = split_data(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class DataSupplier(tf.keras.utils.Sequence):\n",
    "    def __init__(self, batch_size, max_source_seq_len, max_target_seq_len, data, voc_size_source, voc_size_target):\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "        self.voc_size_source = voc_size_source\n",
    "        self.voc_size_target = voc_size_target\n",
    "        self.max_source_seq_len = max_source_seq_len\n",
    "        self.max_target_seq_len = max_target_seq_len\n",
    "        rn.shuffle(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, ndx):\n",
    "        source, target = self.extract_batch(ndx, self.batch_size, self.data)\n",
    "        return self.encode_data(source, target)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        rn.shuffle(self.data)\n",
    "\n",
    "    # secondary auxiliary methods\n",
    "    def encode_data(self, source, target):\n",
    "        encoder_input_data = np.zeros(\n",
    "            (len(source), self.max_source_seq_len, self.voc_size_source), dtype=\"float32\"\n",
    "        )\n",
    "        decoder_input_data = np.zeros(\n",
    "            (len(target), self.max_target_seq_len, self.voc_size_target), dtype=\"float32\"\n",
    "        )\n",
    "        decoder_target_data = np.zeros(\n",
    "            (len(target), self.max_target_seq_len, self.voc_size_target), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        for i, (source_text, target_text) in enumerate(zip(source, target)):\n",
    "            for t, i_vocab in enumerate(source_text.split()):\n",
    "                encoder_input_data[i, t, int(i_vocab)+1] = 1.\n",
    "\n",
    "            # It's maybe a temporary solution\n",
    "            for t in range(len(source_text.split()), self.max_source_seq_len):\n",
    "                encoder_input_data[i, t, 0] = 1.\n",
    "\n",
    "            for t, i_vocab in enumerate(target_text.split()):\n",
    "                decoder_input_data[i, t, int(i_vocab)+1] = 1.\n",
    "                if t > 0:\n",
    "                    decoder_target_data[i, t - 1, int(i_vocab)+1] = 1.\n",
    "\n",
    "            # It's maybe a temporary solution\n",
    "            for t in range(len(target_text.split()), self.max_target_seq_len):\n",
    "                decoder_input_data[i, t, 0] = 1.\n",
    "\n",
    "            # It's maybe a temporary solution\n",
    "            for t in range(len(target_text.split())-1, self.max_target_seq_len):\n",
    "                decoder_target_data[i, t, 0] = 1.\n",
    "\n",
    "        return [encoder_input_data, decoder_input_data], decoder_target_data\n",
    "\n",
    "    @staticmethod\n",
    "    def append_sample(sample, source, target):\n",
    "        source_item, target_item = sample.split('\\t')\n",
    "        source.append(source_item)\n",
    "        target.append(encode_target(target_item))\n",
    "        return source, target\n",
    "\n",
    "    def extract_batch(self, ndx, batch_size, data):\n",
    "        source = []\n",
    "        target = []\n",
    "        ndx_from = ndx * batch_size\n",
    "        ndx_to = min(ndx * batch_size + batch_size, len(data))\n",
    "\n",
    "        for sample in data[ndx_from: ndx_to]:\n",
    "            source, target = self.append_sample(sample, source, target)\n",
    "\n",
    "        if ndx_to % batch_size != 0:\n",
    "            for sample in rn.sample(data[:ndx_from], batch_size - len(data) % batch_size):\n",
    "                source, target = self.append_sample(sample, source, target)\n",
    "\n",
    "        return source, target\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoder_inputs = tf.keras.Input(shape=(None, voc_size_source))\n",
    "bidirectional = tf.keras.layers.Bidirectional\n",
    "encoder = bidirectional(\n",
    "    tf.keras.layers.LSTM(\n",
    "        latent_dim,\n",
    "        return_sequences=True,\n",
    "        return_state=True\n",
    "    )\n",
    ")\n",
    "encoder_stack_h, forward_last_h, forward_last_c, backward_last_h, backward_last_c = encoder(encoder_inputs)\n",
    "\n",
    "encoder_last_h = tf.keras.layers.Concatenate()([forward_last_h, backward_last_h])\n",
    "encoder_last_c = tf.keras.layers.Concatenate()([forward_last_c, backward_last_c])\n",
    "\n",
    "encoder_states = [encoder_last_h, encoder_last_c]\n",
    "\n",
    "decoder_inputs = tf.keras.Input(shape=(None, voc_size_target))\n",
    "\n",
    "decoder = tf.keras.layers.LSTM(latent_dim*2, return_sequences=True, return_state=True)\n",
    "decoder_stack_h, _, _ = decoder(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "context = tf.keras.layers.Attention()([decoder_stack_h, encoder_stack_h])\n",
    "decoder_concat_input = tf.keras.layers.concatenate([context, decoder_stack_h])\n",
    "\n",
    "d0 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(latent_dim, activation=\"relu\"))(decoder_concat_input)\n",
    "dense = tf.keras.layers.Dense(voc_size_target, activation='softmax')\n",
    "decoder_stack_h = tf.keras.layers.TimeDistributed(dense)(d0)\n",
    "\n",
    "model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_stack_h)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.003, amsgrad=True)\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "train_supplier = DataSupplier(\n",
    "    batch_size,\n",
    "    max_source_seq_len,\n",
    "    max_target_seq_len,\n",
    "    data_train,\n",
    "    voc_size_source,\n",
    "    voc_size_target\n",
    ")\n",
    "\n",
    "valid_supplier = DataSupplier(\n",
    "    batch_size,\n",
    "    max_source_seq_len,\n",
    "    max_target_seq_len,\n",
    "    data_valid,\n",
    "    voc_size_source,\n",
    "    voc_size_target\n",
    ")\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "model.fit(\n",
    "    train_supplier,\n",
    "    validation_data=valid_supplier,\n",
    "    epochs=epochs,\n",
    "    shuffle=True,\n",
    "    callbacks=[es]\n",
    ")\n",
    "\n",
    "model.save(\"models/\" + model_name + \".h5\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}