{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import re\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_data = 50*batch_size\n",
    "epochs = 100\n",
    "latent_dim = 128\n",
    "language_tag = 'en'\n",
    "data_path = 'data/' + language_tag + '/original.txt'\n",
    "model_name = 'nmt'\n",
    "validation_split = .2\n",
    "punctuation = ['!', '?', '.', ',']\n",
    "sentinels = ['~', '^']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def encode_target(text):\n",
    "    return '~' + text + '^'\n",
    "\n",
    "\n",
    "def find_max_seq_data_len(data):\n",
    "    max_source_seq_len = 0\n",
    "    max_target_seq_len = 0\n",
    "    for sample in data:\n",
    "        source, target = sample.split('\\t')\n",
    "\n",
    "        source_len = len(source)\n",
    "        if source_len > max_source_seq_len:\n",
    "            max_source_seq_len = source_len\n",
    "\n",
    "        target_len = len(encode_target(target))\n",
    "        if target_len > max_target_seq_len:\n",
    "            max_target_seq_len = target_len\n",
    "\n",
    "    return max_source_seq_len, max_target_seq_len\n",
    "\n",
    "\n",
    "def split_data(data):\n",
    "    data_validation = data[-int(validation_split * len(data)):]\n",
    "    data_train = data[:int(len(data) - len(data_validation))]\n",
    "    return data_train, data_validation\n",
    "\n",
    "\n",
    "def get_voc_from_data(data):\n",
    "    source_voc = list()\n",
    "    for sample in data:\n",
    "        source, _ = sample.split('\\t')\n",
    "        [source_voc.append(token) for token in source if token not in source_voc]\n",
    "    source_voc = sorted(source_voc)\n",
    "    return source_voc, (source_voc + sentinels + punctuation)\n",
    "\n",
    "\n",
    "def split_with_keep_delimiters(string, delimiters):\n",
    "    return re.split('(' + '|'.join(map(re.escape, delimiters)) + ')', string)\n",
    "\n",
    "\n",
    "def get_bi_grams(paired_data, freq):\n",
    "    bi_gram_statistics = dict()\n",
    "    for sample in paired_data:\n",
    "        # a target sample\n",
    "        sample = sample.split('\\t')[1]\n",
    "        tokens = list()\n",
    "        [\n",
    "            [\n",
    "                tokens.append(token) for token in split_with_keep_delimiters(s, punctuation) if token is not ''\n",
    "            ]\n",
    "            for s in sample.split()\n",
    "        ]\n",
    "\n",
    "        for i in range(len(tokens) - 1):\n",
    "            bi_gram = tokens[i] + ' ' + tokens[i + 1]\n",
    "            if (tokens[i] not in punctuation) and (tokens[i + 1] not in punctuation):\n",
    "                if bi_gram not in bi_gram_statistics:\n",
    "                    bi_gram_statistics[bi_gram] = 1\n",
    "                else:\n",
    "                    bi_gram_statistics[bi_gram] += 1\n",
    "\n",
    "    bi_gram_statistics = {k: v for (k, v) in bi_gram_statistics.items() if v > freq}\n",
    "    bi_gram_statistics = {k: v for (k, v) in sorted(bi_gram_statistics.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    return list(bi_gram_statistics.keys())\n",
    "\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    data = f.read().split('\\n')[-num_data:]\n",
    "\n",
    "\n",
    "max_source_seq_len, max_target_seq_len = find_max_seq_data_len(data)\n",
    "rn.shuffle(data)\n",
    "source_voc, target_voc = get_voc_from_data(data)\n",
    "data_train, data_valid = split_data(data)\n",
    "print('len(data):', len(data), 'max_source_seq_len:', max_source_seq_len, 'max_target_seq_len:', max_target_seq_len)\n",
    "print('source_voc:', source_voc)\n",
    "print('target_voc:', target_voc)\n",
    "# print('len(source_voc):', len(source_voc))\n",
    "# print('len(target_voc):', len(target_voc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class DataSupplier(tf.keras.utils.Sequence):\n",
    "    def __init__(self, batch_size, max_source_seq_len, max_target_seq_len, data, source_voc, target_voc):\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "        self.source_voc = source_voc\n",
    "        self.target_voc = target_voc\n",
    "        self.max_source_seq_len = max_source_seq_len\n",
    "        self.max_target_seq_len = max_target_seq_len\n",
    "        rn.shuffle(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, ndx):\n",
    "        source, target = self.extract_batch(ndx, self.batch_size, self.data)\n",
    "        return self.encode_data(source, target)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        rn.shuffle(self.data)\n",
    "\n",
    "    # secondary auxiliary methods\n",
    "    def encode_data(self, source, target):\n",
    "        encoder_input_data = np.zeros(\n",
    "            (len(source), self.max_source_seq_len, len(self.source_voc)), dtype=\"float32\"\n",
    "        )\n",
    "        decoder_input_data = np.zeros(\n",
    "            (len(target), self.max_target_seq_len, len(self.target_voc)), dtype=\"float32\"\n",
    "        )\n",
    "        decoder_target_data = np.zeros(\n",
    "            (len(target), self.max_target_seq_len, len(self.target_voc)), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        for i, (source_text, target_text) in enumerate(zip(source, target)):\n",
    "            for t, symbol in enumerate(source_text):\n",
    "                encoder_input_data[i, t, self.source_voc.index(symbol)] = 1.\n",
    "\n",
    "            for t, symbol in enumerate(target_text):\n",
    "                symbol_ndx = self.target_voc.index(symbol)\n",
    "                decoder_input_data[i, t, symbol_ndx] = 1.\n",
    "                if t > 0:\n",
    "                    decoder_target_data[i, t - 1, symbol_ndx] = 1.\n",
    "\n",
    "        return [encoder_input_data, decoder_input_data], decoder_target_data\n",
    "\n",
    "    @staticmethod\n",
    "    def append_sample(sample, source, target):\n",
    "        source_item, target_item = sample.split('\\t')\n",
    "        source.append(source_item)\n",
    "        target.append(encode_target(target_item))\n",
    "        return source, target\n",
    "\n",
    "    def extract_batch(self, ndx, batch_size, data):\n",
    "        source = []\n",
    "        target = []\n",
    "        ndx_from = ndx * batch_size\n",
    "        ndx_to = min(ndx * batch_size + batch_size, len(data))\n",
    "\n",
    "        for sample in data[ndx_from: ndx_to]:\n",
    "            source, target = self.append_sample(sample, source, target)\n",
    "\n",
    "        if ndx_to % batch_size != 0:\n",
    "            for sample in rn.sample(data[:ndx_from], batch_size - len(data) % batch_size):\n",
    "                source, target = self.append_sample(sample, source, target)\n",
    "\n",
    "        return source, target\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoder_inputs = tf.keras.Input(shape=(None, len(source_voc)))\n",
    "bidirectional = tf.keras.layers.Bidirectional\n",
    "encoder = bidirectional(\n",
    "    tf.keras.layers.LSTM(\n",
    "        latent_dim,\n",
    "        return_sequences=True,\n",
    "        return_state=True\n",
    "    )\n",
    ")\n",
    "encoder_stack_h, forward_last_h, forward_last_c, backward_last_h, backward_last_c = encoder(encoder_inputs)\n",
    "\n",
    "encoder_last_h = tf.keras.layers.Concatenate()([forward_last_h, backward_last_h])\n",
    "encoder_last_c = tf.keras.layers.Concatenate()([forward_last_c, backward_last_c])\n",
    "\n",
    "encoder_states = [encoder_last_h, encoder_last_c]\n",
    "\n",
    "decoder_inputs = tf.keras.Input(shape=(None, len(target_voc)))\n",
    "\n",
    "decoder = tf.keras.layers.LSTM(latent_dim*2, return_sequences=True, return_state=True)\n",
    "decoder_stack_h, _, _ = decoder(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "context = tf.keras.layers.Attention()([decoder_stack_h, encoder_stack_h])\n",
    "decoder_concat_input = tf.keras.layers.concatenate([context, decoder_stack_h])\n",
    "\n",
    "dense = tf.keras.layers.Dense(len(target_voc), activation='softmax')\n",
    "decoder_stack_h = tf.keras.layers.TimeDistributed(dense)(decoder_concat_input)\n",
    "\n",
    "model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_stack_h)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']\n",
    ")\n",
    "\n",
    "train_supplier = DataSupplier(\n",
    "    batch_size,\n",
    "    max_source_seq_len,\n",
    "    max_target_seq_len,\n",
    "    data_train,\n",
    "    source_voc,\n",
    "    target_voc\n",
    ")\n",
    "\n",
    "valid_supplier = DataSupplier(\n",
    "    batch_size,\n",
    "    max_source_seq_len,\n",
    "    max_target_seq_len,\n",
    "    data_valid,\n",
    "    source_voc,\n",
    "    target_voc\n",
    ")\n",
    "\n",
    "# es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "model.fit(\n",
    "    train_supplier,\n",
    "    validation_data=valid_supplier,\n",
    "    epochs=epochs,\n",
    "    shuffle=True,\n",
    "    # callbacks=[es],\n",
    ")\n",
    "\n",
    "model.save(\"models/\" + model_name + \".h5\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}