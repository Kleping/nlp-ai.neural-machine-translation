{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "DISTRIBUTION_DATA_COUNT = [20 * 1000, 40 * 1000, 60 * 1000]\n",
    "VOCABULARY_PUNCTUATION = ['!', '?', '.', ',']\n",
    "DATA_SUFFIXES = ['one', 'two', 'three']\n",
    "SENTINELS = ['^', '~']\n",
    "\n",
    "MODEL_NAME = 'seq2seq_with_attention'\n",
    "MODEL_PATH = 'models/' + MODEL_NAME + '.h5'\n",
    "DATA_NAME = 'data/original.txt'\n",
    "\n",
    "SHIFTED_SEQ_COUNT = 1\n",
    "LATENT_DIMENSIONS = 128\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "MAX_SEQUENCE = 142\n",
    "ACCEPTED_DIFF = .01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_lines(path, formatted):\n",
    "    lines = list()\n",
    "    with open(path, \"r\", encoding='utf-8') as file:\n",
    "        [lines.append(formatted(i)) for i in file.readlines()]\n",
    "    return lines\n",
    "\n",
    "\n",
    "def split_with_keep_delimiters(string, delimiters):\n",
    "    return re.split('(' + '|'.join(map(re.escape, delimiters)) + ')', string)\n",
    "\n",
    "\n",
    "def tokenize_sequence(seq):\n",
    "    return seq.split()\n",
    "\n",
    "\n",
    "def encode_seq(seq, voc):\n",
    "    d_type = 'int32'\n",
    "    arr = np.array([voc.index(seq[i]) for i in range(len(seq))], dtype=d_type)\n",
    "    return np.append(arr, np.zeros(shape=MAX_SEQUENCE - len(seq), dtype=d_type))\n",
    "\n",
    "\n",
    "def decompose_tokens(tokens, shuffle):\n",
    "    decomposed = list()\n",
    "    for i, token in enumerate(tokens):\n",
    "        decomposed.append(tokens[:i+1])\n",
    "    if shuffle:\n",
    "        random.shuffle(decomposed)\n",
    "    return decomposed\n",
    "\n",
    "\n",
    "def clothe_to(str_list, symbols):\n",
    "    new_list = list(str_list)\n",
    "    new_list.insert(0, symbols[0])\n",
    "    new_list.append(symbols[1])\n",
    "    return new_list\n",
    "\n",
    "\n",
    "def seq_to_tokens(seq, voc):\n",
    "    return [voc[seq[i]] for i in range(len(seq))]\n",
    "\n",
    "\n",
    "def decode_seq(input_seq, encoder_model, decoder_model, voc):\n",
    "    encoder_output, encoder_state = encoder_model.predict(np.expand_dims(input_seq, axis=0))\n",
    "    target_seq = np.zeros((1, 1, len(voc)))\n",
    "    target_seq[0, 0, voc.index(SENTINELS[0])] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_tokens = list()\n",
    "    while not stop_condition:\n",
    "        x = len(decoded_tokens) + 1\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq, encoder_output, encoder_state])\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = voc[sampled_token_index]\n",
    "        decoded_tokens.append(sampled_token)\n",
    "\n",
    "        if sampled_token == SENTINELS[1] or len(decoded_tokens) > MAX_SEQUENCE:\n",
    "            stop_condition = True\n",
    "\n",
    "        # target_seq = np.zeros((1, 1, len(voc)))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        encoder_state = [h, c]\n",
    "\n",
    "    return decoded_tokens\n",
    "\n",
    "\n",
    "def linear_regression_equality(y_true, y_pred):\n",
    "    import tensorflow as tf\n",
    "    diff = tf.keras.backend.abs(y_true - y_pred)\n",
    "    return tf.keras.backend.mean(tf.keras.backend.cast(diff < ACCEPTED_DIFF, 'float32'))\n",
    "\n",
    "\n",
    "def get_voc(sequences):\n",
    "    voc = SENTINELS\n",
    "    delimiters = [' ']\n",
    "    for k in sequences:\n",
    "        [[voc.append(w) for w in split_with_keep_delimiters(s, delimiters) if w not in voc] for s in sequences[k]]\n",
    "    voc = sorted(voc)\n",
    "    return voc\n",
    "\n",
    "\n",
    "def split_data(data, coefficient):\n",
    "    validation = list()\n",
    "    train = list()\n",
    "\n",
    "    for k in data:\n",
    "        cluster = data[k]\n",
    "        cluster_len = int(len(cluster) * coefficient // len(data))\n",
    "        [validation.append(i) for i in cluster[-cluster_len:]]\n",
    "        [train.append(i) for i in cluster[:int(len(cluster) - cluster_len)]]\n",
    "\n",
    "    random.shuffle(validation)\n",
    "    random.shuffle(train)\n",
    "    return train, validation\n",
    "\n",
    "\n",
    "def calculate_steps(train, validation):\n",
    "    steps_per_epoch = int(len(train) // BATCH_SIZE)\n",
    "    validation_steps = int(len(validation) // BATCH_SIZE)\n",
    "    return steps_per_epoch, validation_steps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_raw_data(count_coefficient, decompose, assign_max_sequence=False):\n",
    "    global MAX_SEQUENCE\n",
    "    raw_data = dict()\n",
    "    for suffix in DATA_SUFFIXES:\n",
    "        sequences = get_lines('data/normalized/eng_' + suffix + '.txt', lambda l: l[:-1])\n",
    "\n",
    "        if assign_max_sequence:\n",
    "            max_seq = max([len(tokenize_sequence(seq)) for seq in sequences])\n",
    "            if max_seq > MAX_SEQUENCE:\n",
    "                MAX_SEQUENCE = max_seq\n",
    "\n",
    "        if decompose:\n",
    "            sequences_count = len(sequences)\n",
    "            for i in range(sequences_count):\n",
    "                seq = sequences[i]\n",
    "                decomposed_sequences = decompose_tokens(tokenize_sequence(seq), False)[:-1]\n",
    "                [sequences.append(' '.join(tokens)) for tokens in decomposed_sequences]\n",
    "\n",
    "        random.shuffle(sequences)\n",
    "        raw_data[suffix] = sequences[:int(len(sequences) * count_coefficient)]\n",
    "\n",
    "    if assign_max_sequence:\n",
    "        MAX_SEQUENCE += len(SENTINELS) + SHIFTED_SEQ_COUNT\n",
    "        print('assigned_max_sequence(' + str(MAX_SEQUENCE) + ')')\n",
    "\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "def get_fit_data(count_coefficient, split_coefficient):\n",
    "    raw_data = get_raw_data(count_coefficient, decompose=False)\n",
    "    voc = get_voc(raw_data)\n",
    "\n",
    "    train, validation = split_data(raw_data, split_coefficient)\n",
    "    validation_generator = DataSupplier(BATCH_SIZE, validation, voc)\n",
    "    generator = DataSupplier(BATCH_SIZE, train, voc)\n",
    "\n",
    "    print('\\ndata(' + str(len(train)) + ', ' + str(len(validation)) + '),',\n",
    "          'voc_size(' + str(len(voc)) + '),',\n",
    "          'max_sequence(' + str(MAX_SEQUENCE) + ')\\n',\n",
    "          'voc(' + str(voc) + ')\\n')\n",
    "\n",
    "    return (generator, validation_generator), calculate_steps(train, validation), voc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Data 10500/750 19\n",
      "Epoch 1/100\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "class DataSupplier(tf.keras.utils.Sequence):\n",
    "    def __init__(self, batch_size, sentences, voc):\n",
    "        self.batch_size = batch_size\n",
    "        self.sentences = sentences\n",
    "\n",
    "        self.voc_size = len(voc)\n",
    "        self.voc = voc\n",
    "        self.d_type = 'int32'\n",
    "        self.input_length = MAX_SEQUENCE\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.sentences) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        return self.__data_generation(indexes)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.sentences))\n",
    "        np.random.shuffle(self.indexes)\n",
    "\n",
    "    def get_batched_container(self):\n",
    "        return np.zeros((self.batch_size, self.input_length), dtype=self.d_type)\n",
    "\n",
    "    def get_batched_output_container(self):\n",
    "        return np.zeros((self.batch_size, self.input_length, self.voc_size), dtype=self.d_type)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        encoder = self.get_batched_container()\n",
    "        decoder = self.get_batched_container()\n",
    "        output  = self.get_batched_output_container()\n",
    "\n",
    "        cluster = [self.sentences[i] for i in indexes]\n",
    "\n",
    "        for n in range(len(cluster)):\n",
    "            tokens = tokenize_sequence(cluster[n])\n",
    "            encoded_seq = encode_seq(tokens, self.voc)\n",
    "\n",
    "            encoder [n] = encode_seq([i for i in tokens if i not in VOCABULARY_PUNCTUATION], self.voc)\n",
    "            decoder [n] = np.insert(encoded_seq[:-1], 0, self.voc.index(SENTINELS[0]))\n",
    "            pre_output  = np.insert(encoded_seq[:-1], len(tokens), self.voc.index(SENTINELS[1]))\n",
    "\n",
    "            # these arrays are only for test a structure representation of the data\n",
    "            # encoded_test = seq_to_tokens(encoder [n], self.voc)\n",
    "            # decoded_test = seq_to_tokens(decoder [n], self.voc)\n",
    "            # output_test  = seq_to_tokens(pre_output , self.voc)\n",
    "\n",
    "            for p, i in enumerate(pre_output):\n",
    "                if i == 0:\n",
    "                    break\n",
    "                output[n][p][i] = 1.\n",
    "\n",
    "        return [encoder, decoder], output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    model.compile(optimizer='Adamax', loss='categorical_crossentropy', metrics=[linear_regression_equality])\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model(n_voc, n_dim=128, n_emb_dim=128):\n",
    "    encoder_input = tf.keras.layers.Input(shape=(MAX_SEQUENCE,))\n",
    "    encoder_embedding = tf.keras.layers.Embedding(n_voc, n_emb_dim, input_length=MAX_SEQUENCE)(encoder_input)\n",
    "    encoder = tf.keras.layers.LSTM(n_dim, return_sequences=True, return_state=True)\n",
    "    encoder_output, state_h, state_c = encoder(encoder_embedding)\n",
    "    encoder_state = [state_h, state_c]\n",
    "\n",
    "    decoder_input = tf.keras.layers.Input(shape=(MAX_SEQUENCE,))\n",
    "    decoder_embedding = tf.keras.layers.Embedding(n_voc, n_emb_dim, input_length=MAX_SEQUENCE)(decoder_input)\n",
    "    decoder = tf.keras.layers.LSTM(n_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "    # seq2seq\n",
    "    # decoder_output, _, _ = decoder(decoder_embedding, initial_state=encoder_state)\n",
    "    # decoder_dense = tf.keras.layers.Dense(n_voc, activation=\"softmax\")\n",
    "    # output = decoder_dense(decoder_output)\n",
    "\n",
    "    # seq2seq with attention\n",
    "    decoder_output, _, _ = decoder(decoder_embedding, initial_state=encoder_state)\n",
    "    context = tf.keras.layers.Attention()([encoder_output, decoder_output])\n",
    "    decoder_combined_context = tf.keras.layers.Concatenate()([context, decoder_output])\n",
    "    output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_dim, activation=\"relu\"))(decoder_combined_context)\n",
    "    output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_voc, activation=\"softmax\"))(output)\n",
    "\n",
    "    model = tf.keras.Model([encoder_input, decoder_input], output)\n",
    "    model = compile_model(model)\n",
    "    print(model.summary())\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    (train_data, validation_data), (steps_per_epoch, validation_steps), voc = get_fit_data(.15, .2)\n",
    "    model = create_model(n_voc=len(voc), n_dim=LATENT_DIMENSIONS)\n",
    "\n",
    "    model.fit_generator(generator=train_data,\n",
    "                        validation_data=validation_data,\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        validation_steps=validation_steps,\n",
    "                        epochs=EPOCHS,\n",
    "                        verbose=2,\n",
    "                        use_multiprocessing=False,\n",
    "                        shuffle=True)\n",
    "\n",
    "    model.save(MODEL_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}