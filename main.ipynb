{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import keras as k\n",
    "import numpy as np\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "DISTRIBUTION_DATA_COUNT = [20 * 1000, 40 * 1000, 60 * 1000]\n",
    "VOCABULARY_PUNCTUATION = ['!', '?', '.', ',']\n",
    "DATA_SUFFIXES = ['one', 'two', 'three']\n",
    "SENTINELS = ['^', '~']\n",
    "\n",
    "MODEL_NAME = 'seq2seq_with_attention.h5'\n",
    "DATA_NAME = 'data/original.txt'\n",
    "\n",
    "SHIFTED_SEQ_COUNT = 1\n",
    "LATENT_DIMENSIONS = 128\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "MAX_SEQUENCE = 100\n",
    "ACCEPTED_DIFF = .01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_lines(path, formatted):\n",
    "    lines = list()\n",
    "    with open(path, \"r\", encoding='utf-8') as file:\n",
    "        [lines.append(formatted(i)) for i in file.readlines()]\n",
    "    return lines\n",
    "\n",
    "\n",
    "def split_with_keep_delimiters(string, delimiters):\n",
    "    return re.split('(' + '|'.join(map(re.escape, delimiters)) + ')', string)\n",
    "\n",
    "\n",
    "def seq_to_text(encoded_input, voc):\n",
    "    return ''.join([voc[np.argmax(encoded_input[0, i, :])] for i in range(len(encoded_input[0]))])\n",
    "\n",
    "\n",
    "def punctuation_translate(x):\n",
    "    return x.translate({ord(i): None for i in VOCABULARY_PUNCTUATION})\n",
    "\n",
    "\n",
    "def find_max_sequence(samples):\n",
    "    return max([len(sample) for sample in samples])\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq, encoder_model, decoder_model, vocabulary, vocabulary_len):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, vocabulary_len))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, vocabulary.index(SENTINELS[0])] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = vocabulary[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == SENTINELS[1] or len(decoded_sentence) > MAX_SEQUENCE:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, vocabulary_len))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "def linear_regression_equality(y_true, y_pred):\n",
    "    diff = k.backend.abs(y_true - y_pred)\n",
    "    return k.backend.mean(k.backend.cast(diff < ACCEPTED_DIFF, 'float32'))\n",
    "\n",
    "\n",
    "def get_vocabulary(data):\n",
    "    voc = SENTINELS\n",
    "    for k in data:\n",
    "        [[voc.append(w) for w in s.split() if w not in voc] for s in data[k]]\n",
    "    voc = sorted(voc)\n",
    "    voc_size = len(voc)\n",
    "    return voc, voc_size\n",
    "\n",
    "\n",
    "def split_data(data, coefficient):\n",
    "    validation = list()\n",
    "    train = list()\n",
    "\n",
    "    for k in data:\n",
    "        cluster = data[k]\n",
    "        cluster_len = int(len(cluster) * coefficient // len(data))\n",
    "        [validation.append(i) for i in cluster[-cluster_len:]]\n",
    "        [train.append(i) for i in cluster[:int(len(cluster) - cluster_len)]]\n",
    "\n",
    "    random.shuffle(validation)\n",
    "    random.shuffle(train)\n",
    "    return train, validation\n",
    "\n",
    "\n",
    "def calculate_steps(train, validation):\n",
    "    steps_per_epoch = int(len(train) // BATCH_SIZE)\n",
    "    validation_steps = int(len(validation) // BATCH_SIZE)\n",
    "    return steps_per_epoch, validation_steps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class DataSupplier(k.utils.Sequence):\n",
    "    def __init__(self, batch_size, sentences, voc, voc_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.sentences = sentences\n",
    "\n",
    "        self.voc_size = voc_size\n",
    "        self.voc = voc\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.sentences) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        return self.__data_generation(indexes)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.sentences))\n",
    "        np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        encoded_input = np.zeros((self.batch_size, MAX_SEQUENCE, self.voc_size), dtype='float32')\n",
    "        decoded_input = np.zeros((self.batch_size, MAX_SEQUENCE, self.voc_size), dtype='float32')\n",
    "        decoded_output = np.zeros((self.batch_size, MAX_SEQUENCE, self.voc_size), dtype='float32')\n",
    "\n",
    "        cluster = [self.sentences[i] for i in indexes]\n",
    "\n",
    "        for n in range(len(cluster)):\n",
    "            words = (SENTINELS[0] + ' ' + cluster[n] + ' ' + SENTINELS[1]).split()\n",
    "\n",
    "            for i in range(len(words)):\n",
    "                c = self.voc.index(words[i])\n",
    "                # a number of sample, an index of position in the current sentence,\n",
    "                # an index of character in the vocabulary\n",
    "                decoded_output[n, i, c] = 1.\n",
    "\n",
    "                # a number of sample, an index of shifted position in the current sentence,\n",
    "                # an index of character in the vocabulary\n",
    "                decoded_input[n, i + 1, c] = 1.\n",
    "\n",
    "            sentence_without_punctuation = punctuation_translate(cluster[n]).split()\n",
    "\n",
    "            for i in range(len(sentence_without_punctuation)):\n",
    "                c = self.voc.index(sentence_without_punctuation[i])\n",
    "                # a number of sample, an index of position in the current sentence,\n",
    "                # an index of character in the vocabulary\n",
    "                encoded_input[n, i, c] = 1.\n",
    "\n",
    "        return [encoded_input, decoded_input], decoded_output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Data 10500/750 19\n",
      "Epoch 1/100\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def get_data(count_coefficient, shuffle, split_coefficient):\n",
    "    data = dict()\n",
    "    for suffix in DATA_SUFFIXES:\n",
    "        normalized_data = get_lines('data/normalized/eng_' + suffix + '.txt', lambda l: l[:-1])\n",
    "        incomplete_data = get_lines('data/incomplete/eng_' + suffix + '.txt', lambda l: l[:-1])\n",
    "\n",
    "        normalized_data = normalized_data[:int(len(normalized_data) * count_coefficient)]\n",
    "        incomplete_data = incomplete_data[:int(len(incomplete_data) * count_coefficient)]\n",
    "        data[suffix] = normalized_data + incomplete_data\n",
    "\n",
    "    if shuffle is True:\n",
    "        [random.shuffle(data[k]) for k in data]\n",
    "\n",
    "    voc, voc_size = get_vocabulary(data)\n",
    "    # print(max([max([len(sentence.split()) for sentence in data[k]]) for k in data]))\n",
    "\n",
    "    train, validation = split_data(data, split_coefficient)\n",
    "    validation_generator = DataSupplier(BATCH_SIZE, validation, voc, voc_size)\n",
    "    generator = DataSupplier(BATCH_SIZE, train, voc, voc_size)\n",
    "    print('Data', str(len(train)) + '/' + str(len(validation)), voc_size)\n",
    "    return (generator, validation_generator), calculate_steps(train, validation), (voc, voc_size)\n",
    "\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(optimizer='Adamax', loss='categorical_crossentropy', metrics=[linear_regression_equality])\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model(n_input, n_units):\n",
    "    encoder_input = k.Input(shape=(None, n_input,))\n",
    "    encoder = k.layers.LSTM(n_units, return_sequences=True, return_state=True)\n",
    "    encoder_output, state_h, state_c = encoder(encoder_input)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    decoder_input = k.Input(shape=(None, n_input,))\n",
    "    decoder = k.layers.LSTM(n_units, return_sequences=True)\n",
    "    decoder_output = decoder(decoder_input, initial_state=encoder_states)\n",
    "\n",
    "    # decoder_dense = K.layers.Dense(n_input, activation=\"softmax\")\n",
    "    # output = decoder_dense(decoder_output)\n",
    "\n",
    "    attention = k.layers.dot([decoder_output, encoder_output], axes=(2, 2))\n",
    "    attention = k.layers.Activation('softmax', name='attention')(attention)\n",
    "    context = k.layers.dot([attention, encoder_output], axes=(2, 1))\n",
    "    decoder_combined_context = k.layers.concatenate([context, decoder_output])\n",
    "\n",
    "    output = k.layers.TimeDistributed(k.layers.Dense(n_units, activation=\"relu\"))(decoder_combined_context)\n",
    "    output = k.layers.TimeDistributed(k.layers.Dense(n_input, activation=\"softmax\"))(output)\n",
    "\n",
    "    model = k.Model([encoder_input, decoder_input], output)\n",
    "    model = compile_model(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def restore_model():\n",
    "    model = k.models.load_model(MODEL_NAME, compile=False)\n",
    "    return compile_model(model)\n",
    "\n",
    "\n",
    "(train_data, validation_data), (steps_per_epoch, validation_steps), (voc, voc_size) = get_data(.15, True, .2)\n",
    "\n",
    "if os.path.isfile('./' + MODEL_NAME):\n",
    "    model = restore_model()\n",
    "    for i in range(20):\n",
    "        input_data = validation_data.__getitem__(0)[0]\n",
    "        encoded_input = input_data[0]\n",
    "        decoded_input = input_data[1]\n",
    "        input_seq = [encoded_input[i: i + 1], decoded_input[i: i + 1]]\n",
    "        print(seq_to_text(input_seq[0], voc))\n",
    "        output = seq_to_text(model.predict(input_seq), voc)[1:]\n",
    "\n",
    "        for i in range(len(output)):\n",
    "            if output[i] in SENTINELS:\n",
    "                print(output[:i])\n",
    "                print()\n",
    "                break\n",
    "\n",
    "else:\n",
    "    # it's a train process\n",
    "    model = create_model(voc_size, LATENT_DIMENSIONS)\n",
    "\n",
    "    model.fit_generator(generator=train_data,\n",
    "                        validation_data=validation_data,\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        validation_steps=validation_steps,\n",
    "                        epochs=EPOCHS,\n",
    "                        verbose=2,\n",
    "                        use_multiprocessing=False,\n",
    "                        shuffle=True)\n",
    "\n",
    "    model.save(MODEL_NAME)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}