{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "DISTRIBUTION_DATA_COUNT = [20 * 1000, 40 * 1000, 60 * 1000]\n",
    "VOCABULARY_PUNCTUATION = ['!', '?', '.', ',']\n",
    "DATA_SUFFIXES = ['one', 'two', 'three']\n",
    "SENTINELS = ['^', '~']\n",
    "\n",
    "MODEL_NAME = 'seq2seq_with_attention'\n",
    "MODEL_PATH = 'models/' + MODEL_NAME + '.h5'\n",
    "DATA_NAME = 'data/original.txt'\n",
    "\n",
    "SHIFTED_SEQ_COUNT = 1\n",
    "LATENT_DIMENSIONS = 128\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "MAX_SEQUENCE = 142\n",
    "ACCEPTED_DIFF = .01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_lines(path, formatted):\n",
    "    lines = list()\n",
    "    with open(path, \"r\", encoding='utf-8') as file:\n",
    "        [lines.append(formatted(i)) for i in file.readlines()]\n",
    "    return lines\n",
    "\n",
    "\n",
    "def split_with_keep_delimiters(string, delimiters):\n",
    "    return re.split('(' + '|'.join(map(re.escape, delimiters)) + ')', string)\n",
    "\n",
    "\n",
    "def tokenize_sequence(seq):\n",
    "    return seq.split()\n",
    "\n",
    "\n",
    "def encode_seq(seq, voc):\n",
    "    encoded_input = np.zeros((MAX_SEQUENCE, len(voc)), dtype='float32')\n",
    "    for i in range(len(seq)):\n",
    "        c = voc.index(seq[i])\n",
    "        # a number of sample, an index of position in the current sentence,\n",
    "        # an index of character in the vocabulary\n",
    "        encoded_input[i, c] = 1.\n",
    "    return encoded_input\n",
    "\n",
    "\n",
    "def decompose_tokens(tokens, shuffle):\n",
    "    decomposed = list()\n",
    "    for i, token in enumerate(tokens):\n",
    "        decomposed.append(tokens[:i+1])\n",
    "    if shuffle:\n",
    "        random.shuffle(decomposed)\n",
    "    return decomposed\n",
    "\n",
    "\n",
    "def clothe_to(str_list, symbols):\n",
    "    str_list.insert(0, symbols[0])\n",
    "    str_list.append(symbols[1])\n",
    "    return str_list\n",
    "\n",
    "\n",
    "def seq_to_tokens(seq, voc):\n",
    "    return [voc[np.argmax(seq[i, :])] for i in range(len(seq))]\n",
    "\n",
    "\n",
    "def decode_seq(input_seq, encoder_model, decoder_model, voc):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, len(voc)))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, voc.index(SENTINELS[0])] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = voc[sampled_token_index]\n",
    "        decoded_sentence += sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_token == SENTINELS[1] or len(decoded_sentence) > MAX_SEQUENCE:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, len(voc)))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "def linear_regression_equality(y_true, y_pred):\n",
    "    import tensorflow as tf\n",
    "    diff = tf.keras.backend.abs(y_true - y_pred)\n",
    "    return tf.keras.backend.mean(tf.keras.backend.cast(diff < ACCEPTED_DIFF, 'float32'))\n",
    "\n",
    "\n",
    "def get_voc(data):\n",
    "    voc = SENTINELS\n",
    "    delimiters = [' ']\n",
    "    for k in data:\n",
    "        [[voc.append(w) for w in split_with_keep_delimiters(s, delimiters) if w not in voc] for s in data[k]]\n",
    "    voc = sorted(voc)\n",
    "    return voc\n",
    "\n",
    "\n",
    "def split_data(data, coefficient):\n",
    "    validation = list()\n",
    "    train = list()\n",
    "\n",
    "    for k in data:\n",
    "        cluster = data[k]\n",
    "        cluster_len = int(len(cluster) * coefficient // len(data))\n",
    "        [validation.append(i) for i in cluster[-cluster_len:]]\n",
    "        [train.append(i) for i in cluster[:int(len(cluster) - cluster_len)]]\n",
    "\n",
    "    random.shuffle(validation)\n",
    "    random.shuffle(train)\n",
    "    return train, validation\n",
    "\n",
    "\n",
    "def calculate_steps(train, validation):\n",
    "    steps_per_epoch = int(len(train) // BATCH_SIZE)\n",
    "    validation_steps = int(len(validation) // BATCH_SIZE)\n",
    "    return steps_per_epoch, validation_steps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_raw_data(count_coefficient, decompose, assign_max_sequence=False):\n",
    "    global MAX_SEQUENCE\n",
    "    raw_data = dict()\n",
    "    for suffix in DATA_SUFFIXES:\n",
    "        sequences = get_lines('data/normalized/eng_' + suffix + '.txt', lambda l: l[:-1])\n",
    "\n",
    "        if assign_max_sequence:\n",
    "            max_seq = max([len(tokenize_sequence(seq)) for seq in sequences])\n",
    "            if max_seq > MAX_SEQUENCE:\n",
    "                MAX_SEQUENCE = max_seq\n",
    "\n",
    "        if decompose:\n",
    "            sequences_count = len(sequences)\n",
    "            for i in range(sequences_count):\n",
    "                seq = sequences[i]\n",
    "                decomposed_sequences = decompose_tokens(tokenize_sequence(seq), False)[:-1]\n",
    "                [sequences.append(' '.join(tokens)) for tokens in decomposed_sequences]\n",
    "\n",
    "        random.shuffle(sequences)\n",
    "        raw_data[suffix] = sequences[:int(len(sequences) * count_coefficient)]\n",
    "\n",
    "    if assign_max_sequence:\n",
    "        MAX_SEQUENCE += len(SENTINELS) + SHIFTED_SEQ_COUNT\n",
    "        print('assigned_max_sequence(' + str(MAX_SEQUENCE) + ')')\n",
    "\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "def get_fit_data(count_coefficient, split_coefficient):\n",
    "    raw_data = get_raw_data(count_coefficient, decompose=False)\n",
    "    voc = get_voc(raw_data)\n",
    "\n",
    "    train, validation = split_data(raw_data, split_coefficient)\n",
    "    validation_generator = DataSupplier(BATCH_SIZE, validation, voc)\n",
    "    generator = DataSupplier(BATCH_SIZE, train, voc)\n",
    "\n",
    "    print('\\ndata(' + str(len(train)) + ', ' + str(len(validation)) + '),',\n",
    "          'voc_size(' + str(len(voc)) + '),',\n",
    "          'max_sequence(' + str(MAX_SEQUENCE) + ')\\n',\n",
    "          'voc(' + str(voc) + ')\\n')\n",
    "\n",
    "    return (generator, validation_generator), calculate_steps(train, validation), voc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Data 10500/750 19\n",
      "Epoch 1/100\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "class DataSupplier(tf.keras.utils.Sequence):\n",
    "    def __init__(self, batch_size, sentences, voc):\n",
    "        self.batch_size = batch_size\n",
    "        self.sentences = sentences\n",
    "\n",
    "        self.voc_size = len(voc)\n",
    "        self.voc = voc\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.sentences) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        return self.__data_generation(indexes)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.sentences))\n",
    "        np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        encoded_input = np.zeros((self.batch_size, MAX_SEQUENCE, self.voc_size), dtype='float32')\n",
    "        decoded_input = np.zeros((self.batch_size, MAX_SEQUENCE, self.voc_size), dtype='float32')\n",
    "        decoded_output = np.zeros((self.batch_size, MAX_SEQUENCE, self.voc_size), dtype='float32')\n",
    "\n",
    "        cluster = [self.sentences[i] for i in indexes]\n",
    "\n",
    "        for n in range(len(cluster)):\n",
    "            tokens = tokenize_sequence(cluster[n])\n",
    "            tokens_without_punctuation = [i for i in tokens if i not in VOCABULARY_PUNCTUATION]\n",
    "            tokens = clothe_to(tokens, SENTINELS)\n",
    "\n",
    "            decoded_output[n] = encode_seq(tokens, self.voc)\n",
    "\n",
    "            a = np.insert(decoded_output[n], 0, np.zeros(self.voc_size, dtype='float32'))\n",
    "            decoded_input[n] = a[:-self.voc_size].reshape((MAX_SEQUENCE, self.voc_size))\n",
    "\n",
    "            encoded_input[n] = encode_seq(tokens_without_punctuation, self.voc)\n",
    "\n",
    "        return [encoded_input, decoded_input], decoded_output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    model.compile(optimizer='Adamax', loss='categorical_crossentropy', metrics=[linear_regression_equality])\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model(n_input, n_units):\n",
    "    encoder_input = tf.keras.layers.Input(shape=(None, n_input,))\n",
    "    encoder = tf.keras.layers.LSTM(n_units, return_sequences=True, return_state=True)\n",
    "    encoder_output, encoder_state_h, encoder_state_c = encoder(encoder_input)\n",
    "    encoder_state = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "    decoder_input = tf.keras.layers.Input(shape=(None, n_input,))\n",
    "    decoder = tf.keras.layers.LSTM(n_units, return_sequences=True, return_state=True)\n",
    "    decoder_output, decoder_state_h, decoder_state_c = decoder(decoder_input, initial_state=encoder_state)\n",
    "\n",
    "    # seq2seq\n",
    "    # decoder_dense = tf.keras.layers.Dense(n_input, activation=\"softmax\")\n",
    "    # output = decoder_dense(decoder_output)\n",
    "\n",
    "    #seq2seq with attention\n",
    "    context = tf.keras.layers.Attention()([encoder_output, decoder_output])\n",
    "    decoder_combined_context = tf.keras.layers.concatenate([context, decoder_output])\n",
    "    output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_units, activation=\"relu\"))(decoder_combined_context)\n",
    "    output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_input, activation=\"softmax\"))(output)\n",
    "\n",
    "    model = tf.keras.Model([encoder_input, decoder_input], output)\n",
    "    model = compile_model(model)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    (train_data, validation_data), (steps_per_epoch, validation_steps), voc = get_fit_data(.15, .2)\n",
    "    model = create_model(len(voc), LATENT_DIMENSIONS)\n",
    "\n",
    "    model.fit_generator(generator=train_data,\n",
    "                        validation_data=validation_data,\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        validation_steps=validation_steps,\n",
    "                        epochs=EPOCHS,\n",
    "                        verbose=2,\n",
    "                        use_multiprocessing=False,\n",
    "                        shuffle=True)\n",
    "\n",
    "    model.save(MODEL_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}