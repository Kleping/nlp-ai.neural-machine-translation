{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "DISTRIBUTION_DATA_COUNT = [20 * 1000, 40 * 1000, 60 * 1000]\n",
    "VOCABULARY_PUNCTUATION = ['!', '?', '.', ',']\n",
    "DATA_SUFFIXES = ['one', 'two', 'three']\n",
    "SENTINELS = ['^', '~']\n",
    "\n",
    "MODEL_NAME = 'seq2seq_with_attention'\n",
    "MODEL_PATH = 'models/' + MODEL_NAME + '.h5'\n",
    "DATA_NAME = 'data/original.txt'\n",
    "\n",
    "SHIFTED_SEQ_COUNT = 1\n",
    "LATENT_DIMENSIONS = 128\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "MAX_SEQUENCE = 142\n",
    "ACCEPTED_DIFF = .01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_lines(path, formatted):\n",
    "    lines = list()\n",
    "    with open(path, \"r\", encoding='utf-8') as file:\n",
    "        [lines.append(formatted(i)) for i in file.readlines()]\n",
    "    return lines\n",
    "\n",
    "\n",
    "def split_with_keep_delimiters(string, delimiters):\n",
    "    return re.split('(' + '|'.join(map(re.escape, delimiters)) + ')', string)\n",
    "\n",
    "\n",
    "def tokenize_sequence(seq):\n",
    "    return seq.split()\n",
    "\n",
    "\n",
    "def encode_seq(seq, voc):\n",
    "    encoded_input = np.zeros((MAX_SEQUENCE, len(voc)), dtype='float32')\n",
    "    for i in range(len(seq)):\n",
    "        c = voc.index(seq[i])\n",
    "        # a number of sample, an index of position in the current sentence,\n",
    "        # an index of character in the vocabulary\n",
    "        encoded_input[i, c] = 1.\n",
    "    return encoded_input\n",
    "\n",
    "\n",
    "def seq_to_tokens(seq, voc):\n",
    "    return [voc[np.argmax(seq[i, :])] for i in range(len(seq))]\n",
    "\n",
    "\n",
    "def decode_seq(input_seq, encoder_model, decoder_model, voc):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, len(voc)))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, voc.index(SENTINELS[0])] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = voc[sampled_token_index]\n",
    "        decoded_sentence += sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_token == SENTINELS[1] or len(decoded_sentence) > MAX_SEQUENCE:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, len(voc)))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "def linear_regression_equality(y_true, y_pred):\n",
    "    import tensorflow as tf\n",
    "    diff = tf.keras.backend.abs(y_true - y_pred)\n",
    "    return tf.keras.backend.mean(tf.keras.backend.cast(diff < ACCEPTED_DIFF, 'float32'))\n",
    "\n",
    "\n",
    "def get_voc(data):\n",
    "    voc = SENTINELS\n",
    "    delimiters = [' ']\n",
    "    for k in data:\n",
    "        [[voc.append(w) for w in split_with_keep_delimiters(s, delimiters) if w not in voc] for s in data[k]]\n",
    "    voc = sorted(voc)\n",
    "    return voc\n",
    "\n",
    "\n",
    "def split_data(data, coefficient):\n",
    "    validation = list()\n",
    "    train = list()\n",
    "\n",
    "    for k in data:\n",
    "        cluster = data[k]\n",
    "        cluster_len = int(len(cluster) * coefficient // len(data))\n",
    "        [validation.append(i) for i in cluster[-cluster_len:]]\n",
    "        [train.append(i) for i in cluster[:int(len(cluster) - cluster_len)]]\n",
    "\n",
    "    random.shuffle(validation)\n",
    "    random.shuffle(train)\n",
    "    return train, validation\n",
    "\n",
    "\n",
    "def calculate_steps(train, validation):\n",
    "    steps_per_epoch = int(len(train) // BATCH_SIZE)\n",
    "    validation_steps = int(len(validation) // BATCH_SIZE)\n",
    "    return steps_per_epoch, validation_steps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class DataSupplier(tf.keras.utils.Sequence):\n",
    "    def __init__(self, batch_size, sentences, voc):\n",
    "        self.batch_size = batch_size\n",
    "        self.sentences = sentences\n",
    "\n",
    "        self.voc_size = len(voc)\n",
    "        self.voc = voc\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.sentences) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        return self.__data_generation(indexes)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.sentences))\n",
    "        np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        encoded_input = np.zeros((self.batch_size, MAX_SEQUENCE, self.voc_size), dtype='float32')\n",
    "        decoded_input = np.zeros((self.batch_size, MAX_SEQUENCE, self.voc_size), dtype='float32')\n",
    "        decoded_output = np.zeros((self.batch_size, MAX_SEQUENCE, self.voc_size), dtype='float32')\n",
    "\n",
    "        cluster = [self.sentences[i] for i in indexes]\n",
    "\n",
    "        for n in range(len(cluster)):\n",
    "            string = cluster[n]\n",
    "            words = string.split()\n",
    "            words.insert(0, SENTINELS[0])\n",
    "            words.append(SENTINELS[1])\n",
    "\n",
    "            for i in range(len(words)):\n",
    "                c = self.voc.index(words[i])\n",
    "                # a number of sample, an index of position in the current sentence,\n",
    "                # an index of character in the vocabulary\n",
    "                decoded_output[n, i, c] = 1.\n",
    "\n",
    "                # a number of sample, an index of shifted position in the current sentence,\n",
    "                # an index of character in the vocabulary\n",
    "                decoded_input[n, i + 1, c] = 1.\n",
    "\n",
    "            sentence_without_punctuation = [i for i in cluster[n].split() if i not in VOCABULARY_PUNCTUATION]\n",
    "            [sentence_without_punctuation.insert(i, ' ') for i in range(1, len(sentence_without_punctuation)*2 - 1, 2)]\n",
    "            for i in range(len(sentence_without_punctuation)):\n",
    "                c = self.voc.index(sentence_without_punctuation[i])\n",
    "                # a number of sample, an index of position in the current sentence,\n",
    "                # an index of character in the vocabulary\n",
    "                encoded_input[n, i, c] = 1.\n",
    "\n",
    "        return [encoded_input, decoded_input], decoded_output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Data 10500/750 19\n",
      "Epoch 1/100\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def get_raw_data(count_coefficient, shuffle, assign_max_sequence=False):\n",
    "    global MAX_SEQUENCE\n",
    "    raw_data = dict()\n",
    "    for suffix in DATA_SUFFIXES:\n",
    "        normalized_data = get_lines('data/normalized/eng_' + suffix + '.txt', lambda l: l[:-1])\n",
    "        incomplete_data = get_lines('data/incomplete/eng_' + suffix + '.txt', lambda l: l[:-1])\n",
    "\n",
    "        if assign_max_sequence:\n",
    "            concatenated_data = normalized_data + incomplete_data\n",
    "            found_length = max([len(split_with_keep_delimiters(sentence, [' '])) for sentence in concatenated_data])\n",
    "            if found_length > MAX_SEQUENCE:\n",
    "                MAX_SEQUENCE = found_length\n",
    "\n",
    "        if shuffle is True:\n",
    "            random.shuffle(normalized_data)\n",
    "            random.shuffle(incomplete_data)\n",
    "\n",
    "        normalized_data = normalized_data[:int(len(normalized_data) * count_coefficient)]\n",
    "        incomplete_data = incomplete_data[:int(len(incomplete_data) * count_coefficient)]\n",
    "        raw_data[suffix] = normalized_data + incomplete_data\n",
    "\n",
    "    if assign_max_sequence:\n",
    "        MAX_SEQUENCE += len(SENTINELS) + SHIFTED_SEQ_COUNT\n",
    "        print('assigned_max_sequence(' + str(MAX_SEQUENCE) + ')')\n",
    "\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "def get_data(count_coefficient, shuffle, split_coefficient):\n",
    "    raw_data = get_raw_data(count_coefficient, shuffle)\n",
    "    voc = get_voc(raw_data)\n",
    "\n",
    "    train, validation = split_data(raw_data, split_coefficient)\n",
    "    validation_generator = DataSupplier(BATCH_SIZE, validation, voc)\n",
    "    generator = DataSupplier(BATCH_SIZE, train, voc)\n",
    "\n",
    "    print('\\ndata(' + str(len(train)) + ', ' + str(len(validation)) + '),',\n",
    "          'voc_size(' + str(len(voc)) + '),',\n",
    "          'max_sequence(' + str(MAX_SEQUENCE) + ')\\n',\n",
    "          'voc(' + str(voc) + ')\\n')\n",
    "\n",
    "    return (generator, validation_generator), calculate_steps(train, validation), voc\n",
    "\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(optimizer='Adamax', loss='categorical_crossentropy', metrics=[linear_regression_equality])\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model(n_input, n_units):\n",
    "    encoder_input = tf.keras.layers.Input(shape=(None, n_input,))\n",
    "    encoder = tf.keras.layers.LSTM(n_units, return_sequences=True, return_state=True)\n",
    "    encoder_output, state_h, state_c = encoder(encoder_input)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    decoder_input = tf.keras.layers.Input(shape=(None, n_input,))\n",
    "    decoder = tf.keras.layers.LSTM(n_units, return_sequences=True, return_state=True)\n",
    "    decoder_output, _, _ = decoder(decoder_input, initial_state=encoder_states)\n",
    "\n",
    "    # seq2seq\n",
    "    # decoder_dense = tf.keras.layers.Dense(n_input, activation=\"softmax\")\n",
    "    # output = decoder_dense(decoder_output)\n",
    "\n",
    "    #seq2seq with attention\n",
    "    context = tf.keras.layers.Attention()([encoder_output, decoder_output])\n",
    "    decoder_combined_context = tf.keras.layers.concatenate([context, decoder_output])\n",
    "    output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_units, activation=\"relu\"))(decoder_combined_context)\n",
    "    output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_input, activation=\"softmax\"))(output)\n",
    "\n",
    "    model = tf.keras.Model([encoder_input, decoder_input], output)\n",
    "    model = compile_model(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def restore_model(n_units):\n",
    "    model = compile_model(tf.keras.models.load_model(MODEL_PATH, compile=False))\n",
    "    print(model.summary())\n",
    "\n",
    "    encoder_input = model.input[0]\n",
    "    encoder_output, encoder_h, encoder_c = model.layers[1].output\n",
    "    encoder_state = [encoder_h, encoder_c]\n",
    "    encoder_model = tf.keras.Model(encoder_input, encoder_state)\n",
    "\n",
    "    decoder_input = model.input[1]\n",
    "    decoder = model.layers[3]\n",
    "    decoder_new_h = tf.keras.Input(shape=(n_units,), name='input_3')\n",
    "    decoder_new_c = tf.keras.Input(shape=(n_units,), name='input_4')\n",
    "    decoder_input_initial_state = [decoder_new_h, decoder_new_c]\n",
    "\n",
    "    decoder_output, decoder_h, decoder_c = decoder(decoder_input, initial_state=decoder_input_initial_state)\n",
    "    decoder_output_state = [decoder_h, decoder_c]\n",
    "\n",
    "    # context = model.layers[4]([encoder_output, decoder_output])\n",
    "    # decoder_combined_context = model.layers[5]([context, decoder_output])\n",
    "    # output = model.layers[6](decoder_combined_context)\n",
    "    dec_output = model.layers[7](decoder_output)\n",
    "\n",
    "    decoder_model = tf.keras.Model([decoder_input] + decoder_input_initial_state, [dec_output] + decoder_output_state)\n",
    "    return encoder_model, decoder_model\n",
    "\n",
    "\n",
    "def inference_model():\n",
    "    raw_data = get_raw_data(1., True)\n",
    "    voc = get_voc(raw_data)\n",
    "    encoder_model, decoder_model = restore_model(LATENT_DIMENSIONS)\n",
    "\n",
    "    for sentence in random.choices(sum(raw_data.values(), []), k=10):\n",
    "        input_data = encode_sentence(sentence, voc)\n",
    "        output = decode_sequence(input_data, encoder_model, decoder_model, voc)\n",
    "        print('\\ninput ' + sentence, '\\noutput ' + output)\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    (train_data, validation_data), (steps_per_epoch, validation_steps), voc = get_data(.05, True, .2)\n",
    "    model = create_model(len(voc), LATENT_DIMENSIONS)\n",
    "\n",
    "    model.fit_generator(generator=train_data,\n",
    "                        validation_data=validation_data,\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        validation_steps=validation_steps,\n",
    "                        epochs=EPOCHS,\n",
    "                        verbose=2,\n",
    "                        use_multiprocessing=False,\n",
    "                        shuffle=True)\n",
    "\n",
    "    model.save(MODEL_PATH)\n",
    "\n",
    "\n",
    "def get_concrete_function():\n",
    "    import tensorflow as tf\n",
    "\n",
    "    model = tf.keras.models.load_model(MODEL_PATH, compile=False)\n",
    "    compile_model(model)\n",
    "\n",
    "    full_model = tf.function(lambda x: model(x))\n",
    "\n",
    "    x_tensor_spec = tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype)\n",
    "    y_tensor_spec = tf.TensorSpec(model.inputs[1].shape, model.inputs[1].dtype)\n",
    "\n",
    "    return full_model.get_concrete_function(x=[x_tensor_spec, y_tensor_spec])\n",
    "\n",
    "\n",
    "def convert_to_tf_lite(path):\n",
    "    import tensorflow as tf\n",
    "    cf = get_concrete_function()\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_concrete_functions([cf])\n",
    "    lite_model = converter.convert()\n",
    "\n",
    "    with tf.io.gfile.GFile(path, 'wb') as f:\n",
    "        f.write(lite_model)\n",
    "\n",
    "\n",
    "def write_graph():\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "    full_model = get_concrete_function()\n",
    "    frozen_func = convert_variables_to_constants_v2(full_model)\n",
    "    frozen_func.graph.as_graph_def()\n",
    "\n",
    "    tf.io.write_graph(graph_or_graph_def=frozen_func.graph,\n",
    "                      logdir=\"./models\",\n",
    "                      name=MODEL_NAME + '.pb',\n",
    "                      as_text=False)\n",
    "\n",
    "\n",
    "def read_graph():\n",
    "    import tensorflow as tf\n",
    "    with tf.io.gfile.GFile('./models/' + MODEL_NAME + '.pb', \"rb\") as f:\n",
    "        graph_def = tf.compat.v1.GraphDef()\n",
    "        loaded = graph_def.ParseFromString(f.read())\n",
    "    return loaded\n",
    "\n",
    "train_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}